% !TeX root = holoclean.tex
% !TeX encoding = UTF-8
% !TeX spellcheck = en_US

\section{Data Repairing}\label{sec:introduction}
  Data is a central aspect in most enterprises.
  It is the basis for operational and strategic business decision making.
  However, real world data is flawed and contains errors which can impair reporting, customer service and production facilitation.
  Studies show that poor data quality leads to costs of billions of dollars~\cite{Redman:quality_disaster, cost_of_low_qual}.
  Maintaining the quality and consistency of business data has therefore become a critical task.

  One way to improve data quality is data cleaning.
  It consists of two steps: Error detection and data repairing.
  Error detection describes to process of identifying incorrect values in a dataset.
  Subsequently data repairing transforms the erroneous dataset into a new one removing detected errors, so the new dataset adheres to data quality requirements.

  \bigskip
  \textbf{Related Work:}
  Many researchers have made efforts to automate the task of error detection.
  \begin{itemize}
    \item \textbf{TBD:}
    \item Most of the approaches try to detect the violation of \glspl{ic}.
    \item duplicate detection
    \item outlier detection
  \end{itemize}

  Data repairing techniques can be classified according to whether and how humans are involved in the repairing process.
  On the one end there are fully automatic approaches like SCARE that leverage machine learning and intelligent partition algorithms to repair a database without the need of user input~\cite{scare}.
  On the other end there are data wrangling tools that make extensive use of human knowledge to clean a dirty dataset.
  For example, Data Wrangler~\cite{data_wrangler}, the successor of Potter's Wheel~\cite{potters_wheel}, provides a visual interface for cleaning a dataset.
  Based on data statistics it creates visual suggestions for data transformations.
  The user selects and executes those transformations to create a cleaned dataset instance.
  The tool can export the transformation sequence, to repeat the process on another dataset.
  This makes it an appropriate tool for the manual definition of ETL processes.
  The final version of the tool, Trifacta Wrangler~\cite{trifacta_wrangler}, is developed commercially.

  All those tools~\cite{scare,potters_wheel,data_wrangler,trifacta_wrangler} use quantitative statistics of the dataset itself to repair errors in it.
  Besides that, state-of-the-art methods also use \glspl{ic}~\cite{ajax,gdr,editing_rules,data_tamer} and external knowledge~\cite{katara}, like dictionaries, knowledge bases or domain expert annotations, as input signals.
  Figure~\ref{fig:tools} shows those tools aligned on their level of user interaction needed to perform data repairing and the usage of the three different input signals.

  \begin{figure*}
    \centering
    \input{tikz/tools}
    \caption{Data Repairing Tools in Context}
    \label{fig:tools}
  \end{figure*}

  \bigskip
  Most of the tools limit themselves to only one signal to perform data repairing, ignoring other information sources.
  Each type of signal is associated with a different action on the dataset and has its own downsides.
  Data repairs that use \glspl{ic} could introduce incorrect repairs, because they are relying on the \textit{minimality} principle and assume that most of the data values are clean.
  This is not necessarily the case and minimal repairs not always correspond to correct repairs~\cite{holoclean}.
  Repair algorithms relying on external information are dependent on the coverage of the external data source and can therefore perform poorly.
  Quantitative statistics heavily depend on the available information in the dataset itself.
  For small datasets and unfortunate situations this can drastically decrease repairing quality.
  
  \citeauthor{holoclean} therefore propose a holistic data repairing approach that includes all aforementioned signals into one framework: \holoclean{}~\cite{holoclean}.
  To tackle the problem that different signals could suggest conflicting repairs they convert all signals into features of a probabilistic model.
  Based on this combined view on the data and their inconsistencies they can then use statistical learning and probabilistic inference to suggest repairs for the dataset.
  
  \begin{itemize}
    \item \textbf{Paper Preview/Overview}
  \end{itemize}


\section{Background}\label{sec:background}
  In the following paragraphs we review the terminology used in the next sections.
  
  \subsection{Integrity Constraints}
  Users of the \holoclean{} framework can specify \glspl{ic} in form of denial constraints.
  They combine different types of integrity definitions.
  Functional Dependencies as well as conditional functional dependencies can be expressed as denial constraints~\cite{fd_to_dc}.
  A denial constraint states that all predicates of it cannot be true at the same time, otherwise, there is a consistency conflict.
  It is notated as $\forall t_i, t_j \in D: \neg(P_1 \wedge \dots \wedge P_K)$.
  Over all tuples $t \in D$ of a database instance $D$ not all predicates $P_k$ are allowed to be true.
  $P_k$ are defined as $v_1 \circ v_2$ or $v_1 \circ c$ with $v_1, v_2 \in \{t_i.A, t_j.A\}$, where $A$ denotes an attribute of the dataset $D$.
  The operator $\circ$ can be one of $\{=,<,>,\neq,\leq,\geq\}$~\cite{chu2013discoveringdc}.
  In this report we use functional dependencies of the form $t_i.A_1 \rightarrow t_j.A_2$ to express integrity constraints for readability reasons.
  They can easily converted to denial constraints~\cite{fd_to_dc}.
  
  \subsection{\deepdive{} and Factor Graphs}
  \holoclean{} relies on \deepdive{} to perform statistical learning and inference.
  It is a data management system developed at Stanford University, which allows scalable statistical inference on big unclean datasets~\cite{deepdive}.
  \holoclean{} uses \deepdive{} to create a factor graph over all input signals and data cells.
  The factor graph can be defined via a declarative language, called \ddlog{}.
  A collection of inference rules in \deepdive{} corresponds to a probabilistic model.
  Such inference rules are defined over a relation of random variables with weight annotations.
  We consider the following \ddlog{} rule as a template for the inference rules generated by the \holoclean{} framework:
  \begin{multline}
    Value?(t,a,d):-\\Relation(t,a,d), [\text{conditions}], weight=\text{w}
  \end{multline}
  $Value?(t,a,d)$ is the head of the rule and defines a random variable identified by $t$, $a$ and $d$.
  \holoclean{} uses $t$ to identify a tuple and $a$ to identify an attribute.
  $d$ corresponds to the value of cell $t.a$.
  The body of the rule consists of three parts:
  A number of other relations over the used variables (here only one: $Relation(t,a,d)$),
  conditions using the same operators like denial constraints enclosed in brackets and
  a weight annotation ($weight=\text{w}$).
  Grounding of those rules will generate the nodes and factors of the factor graph in \deepdive{}.
  

\section{The \holoclean{} Framework}\label{sec:framework}
  Figure~\ref{fig:architecture} shows an overview over the framework's architecture.
  It takes as input a dirty dataset and repairing constraints.
  Those are \glspl{ic} in form of denial constraints and external clean information in form of matching dependencies and a dictionary.
  Matching dependency map values of dataset cells to the corresponding entries in an external clean dictionary.
  As \holoclean{} treats error detection as a black box, the user has to specify error detection algorithms as a side-input.
   
  \begin{figure}
    \centering
    \input{tikz/architecture}
    \caption{\holoclean{} architecture overview}
    \label{fig:architecture}
  \end{figure}

  \holoclean{} outputs repair suggestions for all cells, that were not marked as clean.
  For each noisy cell it computes the marginal probabilities of all values of its domain.
  For example, if the cell $t_1.a$ is assigned value $\hat{v}$ with a probability of $0.73$, this means that \holoclean{} is $73\%$ confident about this repair.
  In a second step, \holoclean{} repairs the noisy cells by assigning it to the value with the highest probability.
  
  To compute these repairs, \holoclean{} performs three consecutive steps: \textsf{Error Detection}, \textsf{Probabilistic Model Generation} and \textsf{Data Repairing}.
  They are described in the next sections in more detail.
 
  \subsection{Error Detection}
  As a first step \holoclean{} has to separate clean cells from potentially dirty cells of the dataset $D$.
  \holoclean{} treats this error detection step as a black box and the user can specify and use any method to separate dirty and clean cells of each other.
  The framework already provides exemplary algorithms that use denial constraints, outlier detection or external labeled data.
  It is possible to use several detection algorithms at once.
  In this case the final set of dirty cells $D_d$ is the union of all sets of dirty cells and the final set of clean cells $D_c$ is set to $D_c = D \setminus D_d$.

  \subsection{Probabilistic Model Generation}
  \holoclean{} generates a probabilistic program written in \ddlog{}, which is executed using \deepdive{}.
  This program describes a graphical probabilistic model over all cells of the input database and the input signals.
  
  Each cell in the dataset $D$ is assigned to a random variable, which can take values of a finite domain representing the domain of its attribute.
  The random variables are linked via repairing constraints, which are derived from the various input signals.
  They express the uncertainty over the values of noisy cells.
  \holoclean{} uses a factor graph to represent this probabilistic model and perform computations on it.
  
  The compilation process consists of three steps which are executed consecutively:
  \begin{enumerate}
    \item Generate \ddlog{} relations.
    \item Use those relations to form inference rules.
    \item Use \deepdive{} to ground the model, yielding the factor graph.
  \end{enumerate}
    
  First \holoclean{} generates \ddlog{} relations, which encode information about the dataset $D$.
  The relations can be obtained by simple transformations of the original dataset and are shown in table~\ref{tab:relations}.
  Variable $t$ identifies tuples in the dataset $D$ and $a$ identifies attributes in $D$.
  Relation 5 (\ddrule{ExtDict}) is optional and only generated if there is an external dictionary provided by the user.
  \holoclean{} creates a categorical random variable for each cell $t.a$ in the dataset:
  \begin{equation}
    Value?(t,a,d):-Domain(t,a,d)\label{equ:cells}
  \end{equation}
  Afterwards the \ddlog{} relations are used to form inference rules over the random variables.
  
  \begin{table}
    \caption{Generated \ddlog{} Relations in \holoclean{}. Obtained from~\cite{holoclean}}
    \label{tab:relations}
    \begin{tabular}{rlp{0.6\linewidth}@{}}
      \toprule
      & Relation & Description\\
      \midrule
      1 & \ddrule{Tuple(t)} & Relation of all tuple identifiers.\\
      2 & \ddrule{InitValue(t,a,v)} & Maps a cell $t.a$ to its initial value $v$ in the input dataset.\\
      3 & \ddrule{Domain(t,a,d)} & Maps a cell $t.a$ to its domain $d \in dom(t.a)$.\\
      4 & \ddrule{HasFeature(t,a,f)} & Maps a cell $t.a$ to a feature vector $f$.\\
      5 & \ddrule{ExtDict(t\textsubscript{k},a\textsubscript{k},v,k)} & Maps a dictionary cell $t_k.a_k$ to its value $v$ in the dictionary $k$.\\
      \bottomrule
    \end{tabular}
  \end{table}
  
  \subsubsection*{Minimality Principle}
  \holoclean{} uses the minimality principle as the prior that the dataset consists of more clean cells than erroneous ones.
  This is encoded in the following rule, which maps each cell's value to its initial value in the input dataset:
  \begin{equation}
    Value?(t,a,d):-InitValue(t,a,d)\ weight=w\label{equ:minimality}
  \end{equation}
  $w$ is constant and represents the strength of this prior.
  
  \subsubsection*{Quantitative Statistics}
  Relation \ddrule{HasFeature} is used to capture quantitative statistics of the input dataset:
  \begin{multline}
    Value?(t,a,d):-\\HasFeature(t,a,f)\ weight=w(d,f)\label{equ:statistics}
  \end{multline}
  To encode the co-occurrences of attribute values \ddrule{HasFeature} stores the values of other cells in the same tuple of a given cell.
  Users can add more features to \ddrule{HasFeature}.
  This allows for example to track the provenance and prioritize data sources differently.
  \holoclean{} parametrizes the weight annotation for the feature inference rule by the value of the cell $d$ and the feature vector $f$ to allow different confidence levels for the combinations of the values of $d$ and $f$.
  Those weights are not set during the compilation of the rules itself and can be learned later on.
  
  \subsubsection*{Integrity Constraints}
  \begin{multline}
    !
    \begin{pmatrix}
      Value?(t_1,DBAName,d_1) \land\\
      Value?(t_2,DBAName, d)_2 \land\\
      Value?(t_1,FacilityType,f_1) \land\\
      Value?(t_2,FacilityType,f_2)
    \end{pmatrix}
    :-\\
    Tuple(t_1),Tuple(t_2)[d_1 = d_2, f_1 \neq f_2]\ weight=w\label{equ:ic}
  \end{multline}
  
  \subsubsection*{External Information}
  \begin{equation}
    Value(t,a,d):-Matched(t,a,d,k)\ weight=w(k)\label{equ:matching}
  \end{equation}
  
  \begin{itemize}
    \item result = \ddlog{} relations and inference rules, some weights constant, some variable
    \item grounding generates all random variables and the factors
  \end{itemize}

  
  \subsection{Data Repairing}
  \begin{itemize}
    \item now: clean and dirty cells as training data for statistical learning of factor graph weights
    \item Bayes inference to obtain marginal probabilities = distribution of all values of a random variable (for all randvars)
    \item approximate approach using gibbs sampling~\cite{gibbssampling}
    \item estimates values $\hat{v}$ of dirty cells
    \item maximum a posteriori estimates are used to repair dataset
  \end{itemize}

\section{Performance of \holoclean{}}\label{sec:performance}

\section{Conclusion}\label{sec:conclusion}