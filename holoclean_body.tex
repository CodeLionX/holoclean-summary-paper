% !TeX root = holoclean.tex
% !TeX encoding = UTF-8
% !TeX spellcheck = en_US

\section{Data Repairing}\label{sec:introduction}
  Data is a central aspect in most enterprises.
  It is the basis for operational and strategic business decision making.
  However, real world data is flawed and contains errors, which can impair reporting, customer service, and production facilitation.
  Studies show that poor data quality leads to costs of billions of dollars~\cite{Redman:quality_disaster, cost_of_low_qual}.
  Maintaining the quality and consistency of business data has therefore become a critical task.

  \subsection{Data cleaning}
  One way to improve data quality is data cleaning.
  It consists of two steps: Error detection and data repairing.
  Error detection describes the process of identifying incorrect values in a dataset.
  Subsequently data repairing transforms the erroneous dataset into a new one removing detected errors, so the new dataset adheres to data quality requirements.

  \subsection{Related work}
  Many researchers have made efforts to automate the task of error detection.
  \begin{itemize}
    \item \textbf{TODO:}
    \item Most of the approaches try to detect the violation of \glspl{ic}.
    \item duplicate detection
    \item outlier detection
  \end{itemize}

  Data repairing techniques can be classified according to whether and how humans are involved in the repairing process.
  On the one end there are fully automatic approaches, such as SCARE~\cite{scare}, that leverage machine learning and intelligent partition algorithms to repair a database without the need of user input.
  On the other end there are data wrangling tools that make extensive use of human knowledge to clean a dirty dataset.
  For example, Data Wrangler~\cite{data_wrangler}, the successor of Potter's Wheel~\cite{potters_wheel}, provides a visual interface for cleaning a dataset.
  Based on data statistics it creates visual suggestions for data transformations.
  The user selects and executes those transformations to create a cleaned dataset instance.
  The tool can export the transformation sequence, to repeat the process on another dataset.
  This makes it an appropriate tool for the manual definition of ETL processes.
  The final version of the tool, Trifacta Wrangler~\cite{trifacta_wrangler}, is developed commercially.

  All those tools~\cite{scare,potters_wheel,data_wrangler,trifacta_wrangler} use quantitative statistics of the dataset itself to repair errors in it.
  Besides that, state-of-the-art methods also use \glspl{ic}~\cite{holistic,ajax,gdr,editing_rules,data_tamer} and external knowledge~\cite{katara}, such as dictionaries, knowledge bases, or domain expert annotations, as input signals.
  Figure~\ref{fig:tools} shows those tools aligned on their level of user interaction needed to perform data repairing and the usage of the three different input signals.

  \begin{figure*}
    \centering
    \includestandalone{tikz/tools}
    %\includestandalone{tikz/tools_venn}
    \caption{Data Repairing Tools in Context}
    \label{fig:tools}
  \end{figure*}

  %\input{tools_matrix}

  \subsection{Introducing \holoclean{}}
  Most of the tools limit themselves to only one signal to perform data repairing, ignoring other information sources.
  Each type of signal is associated with a different action on the dataset and has its own downsides.
  Data repairs that use \glspl{ic} could introduce incorrect repairs, because they are relying on the \textit{minimality principle} and assume that most of the data values are clean.
  This is not necessarily the case and minimal repairs not always correspond to correct repairs~\cite{holoclean}.
  Repair algorithms relying on external information are dependent on the coverage of the external data source and can therefore perform poorly.
  Quantitative statistics heavily depend on the available information in the dataset itself.
  For small datasets and unfortunate situations this can drastically decrease repairing quality.
  
  \citeauthor{holoclean} therefore propose a holistic data repairing approach that includes all aforementioned signals into one framework: \holoclean{}~\cite{holoclean}.
  To tackle the problem that different signals could suggest conflicting repairs they convert all signals into features of a probabilistic model.
  Based on this combined view on the data and their inconsistencies they can then use statistical learning and probabilistic inference to suggest repairs for the dataset.
  
  \subsection{Overview}
  \begin{itemize}
    \item following work, we explain \holoclean{} in more detail
    \item describe chapters
  \end{itemize}


\section{Background}\label{sec:background}
  In the following paragraphs we review the terminology used in the next sections.
  
  \subsection{Integrity constraints}
  Users of the \holoclean{} framework can specify \glspl{ic} in form of denial constraints, which combine different types of integrity definitions.
  Functional Dependencies as well as conditional functional dependencies can be expressed as denial constraints~\cite{fd_to_dc}.
  A denial constraint states that all predicates of it cannot be true at the same time, otherwise, there is a consistency conflict.
  It is notated as $\forall t_i, t_j \in D: \neg(P_1 \wedge \dots \wedge P_K)$.
  Over all tuples $t \in D$ of a database instance $D$ not all predicates $P_k$ are allowed to be true.
  $P_k$ are defined as $v_1 \circ v_2$ or $v_1 \circ c$ with $v_1, v_2 \in \{t_i.A, t_j.A\}$, where $A$ denotes an attribute of the dataset $D$.
  The operator $\circ$ can be one of $\{=,<,>,\neq,\leq,\geq\}$~\cite{chu2013discoveringdc}.
  For readability reasons we use functional dependencies of the form $A_1 \to A_2$ to express integrity constraints in this report.
  They can easily be converted to denial constraints~\cite{fd_to_dc}.
  
  \subsection{Factor graphs in \deepdive{}}
  \holoclean{} relies on \deepdive{} to perform statistical learning and inference.
  It is a data management system developed at Stanford University, which allows scalable statistical inference on big unclean datasets~\cite{deepdive}.
  \holoclean{} uses \deepdive{} to create a factor graph~\cite{pgmFactorGraph} over all input signals and data cells.
  The factor graph can be defined via a declarative language, called \ddlog{}.
  A collection of inference rules in \deepdive{} corresponds to a probabilistic model.
  Such inference rules are defined over a relation of random variables with weight annotations.
  We consider the following \ddlog{} rule as a template for the inference rules generated by the \holoclean{} framework:
  \begin{multline}
    Value?(t,a,d):-\\Relation(t,a,d), [\text{conditions}], weight=\text{w}
  \end{multline}
  $Value?(t,a,d)$ is the head of the rule and defines a random variable identified by $t$, $a$ and $d$.
  \holoclean{} uses $t$ to identify a tuple and $a$ to identify an attribute.
  $d$ corresponds to the value of cell $t.a$.
  The body of the rule consists of three parts:
  A number of other relations over the used variables (here only one: $Relation(t,a,d)$),
  conditions using the same operators, such as denial constraints, enclosed in brackets, and
  a weight annotation ($weight=\text{w}$).
  Grounding of those rules will generate the nodes and factors of the factor graph in \deepdive{}.
  

\section{The \holoclean{} Framework}\label{sec:framework}
  Figure~\ref{fig:architecture} shows an overview over the framework's architecture.
  It takes as input a dirty dataset and repairing constraints.
  Those are \glspl{ic} in form of denial constraints and external clean data in form of matching dependencies and a dictionary.
  Matching dependency map values of the unclean dataset cells to the corresponding entries in an external clean dictionary.
  As \holoclean{} treats error detection as a black box, the user has to specify error detection algorithms as a side-input.
   
  \begin{figure}
    \centering
    \includestandalone{tikz/architecture}
    \caption{\holoclean{} Architecture Overview}
    \label{fig:architecture}
  \end{figure}

  \holoclean{} outputs repair suggestions for all cells which were not marked as clean.
  For each noisy cell it computes the marginal probabilities of all values of its domain.
  For example, if the cell $t_1.a$ is assigned value $\hat{v}$ with a probability of $0.73$, this means that \holoclean{} is $73\%$ confident about this repair.
  In a second step, \holoclean{} repairs the noisy cells by assigning it to the value suggestions with the highest probability.
  
  To compute these repairs, \holoclean{} performs three consecutive steps: \textsf{Error Detection}, \textsf{Probabilistic Model Generation}, and \textsf{Data Repairing}.
  They are described in the next sections in more detail.
 
  \subsection{Error detection}
  As a first step \holoclean{} has to separate clean cells from potentially dirty cells of the dataset $D$.
  \holoclean{} treats this error detection step as a black box and the user can specify and use any method to separate dirty and clean cells from each other.
  The framework already provides exemplary algorithms that use denial constraints, outlier detection, or external labeled data for the separation of the cells.
  It is possible to use several detection algorithms at once.
  In this case the final set of dirty cells $D_d$ is the union of all sets of dirty cells and the final set of clean cells $D_c$ is the set $D_c = D \setminus D_d$.

  \subsection{Probabilistic model generation}
  \holoclean{} generates a probabilistic program written in \ddlog{}, which is executed using \deepdive{}.
  This program describes a graphical probabilistic model over all cells of the input database and the input signals.
  
  Each cell in the dataset $D$ is assigned to a random variable, which can take values of a finite domain.
  The random variables are linked via repairing constraints, which are derived from the various input signals.
  They express the uncertainty over the values of noisy cells.
  \holoclean{} uses a factor graph to represent this probabilistic model and perform computations on it.
  
  The compilation process consists of three steps which are executed consecutively:
  \begin{enumerate}
    \item Generate \ddlog{} relations.
    \item Use those relations to form inference rules.
    \item Use \deepdive{} to ground the model, yielding the factor graph.
  \end{enumerate}
    
  First \holoclean{} generates \ddlog{} relations, which encode information about the dataset $D$.
  They can be obtained by simple transformations of the original dataset and are shown in table~\ref{tab:relations}.
  Variable $t$ identifies tuples in the dataset $D$ and $a$ identifies attributes in $D$.
  Relation 5 (\ddrule{ExtDict}) is optional and only generated if there is an external dictionary provided by the user.
  \holoclean{} creates a categorical random variable for each cell $t.a$ in the dataset using relation 3 (\ddrule{Domain}):
  \begin{equation}
    Value?(t,a,d):-Domain(t,a,d)\label{equ:cells}
  \end{equation}
  Those random variables are part of the factors that encode repairing constraints in the final factor graph.
  During the generation of the factor graph, large domains of those random variables can lead to a combinatorial explosion~\cite{pgm}.
  %TODO differnce between evidence and query variables
  Instead of assigning the whole domain of the corresponding attribute in the original dataset, \holoclean{} prunes the domain of the random variables in the relation \ddrule{Domain}.
  It only considers those values of a cell for relation \ddrule{Domain}, that exist in the cell's domain and co-occur with other values of the cell's tuple with a probability greater than a threshold $\tau$.

  This means for each cell $t.A \in D$ \holoclean{} creates a new value-set $Y_{t.A}$ only containing those values $v \in dom(A)$ with a co-occurrence probability higher than the threshold $P(v|v') \geq \tau$.
  The co-occurrence probability for a value $v$ of cell $t.A$ is computed based on $v$ and the value of all other cells in $t.A$'s tuple $v' \in \{\forall a \in D\setminus\{A\}: v_{t.a}\}$:
  \begin{equation}
    P(v|v') = \frac{\#(v_i,v')\,\text{appear together in}\, D}{\#v'\,\text{appears in}\, D}~\cite{holoclean}
  \end{equation}
  The threshold $\tau$ allows a trade-off between the quality of repairs and the runtime of \holoclean{} for big datasets.
  It controls the size of the random variable's domain and therefore the size of the grounded factor graph.

  After definition of the random variables, the \ddlog{} relations are used to form inference rules over the random variables:
  
  \begin{table}
    \caption{Generated \ddlog{} Relations in \holoclean{}. Obtained from~\cite{holoclean}}
    \label{tab:relations}
    \begin{tabular}{rlp{0.6\linewidth}@{}}
      \toprule
      & Relation & Description\\
      \midrule
      1 & \ddrule{Tuple(t)} & Relation of all tuple identifiers.\\
      2 & \ddrule{InitValue(t,a,v)} & Maps a cell $t.a$ to its initial value $v$ in the input dataset.\\
      3 & \ddrule{Domain(t,a,d)} & Maps a cell $t.a$ to its finite domain $d \in Y_{t.a}$.\\
      4 & \ddrule{HasFeature(t,a,f)} & Maps a cell $t.a$ to a feature vector $f$.\\
      5 & \ddrule{ExtDict(t\textsubscript{k},a\textsubscript{k},v,k)} & Maps a dictionary cell $t_k.a_k$ to its value $v$ in the dictionary $k$.\\
      \bottomrule
    \end{tabular}
  \end{table}
  
  \subsubsection*{Minimality principle}
  \holoclean{} uses the minimality principle as the prior that the dataset consists of more clean cells than erroneous ones.
  This is encoded in the following rule, which maps each cell's value to its initial value in the input dataset:
  \begin{equation}
    Value?(t,a,d):-InitValue(t,a,d)\ weight=w\label{equ:minimality}
  \end{equation}
  The weight $w$ is set to a constant value during compilation of the \ddlog{} rules and represents the strength of this prior.
  
  \subsubsection*{Quantitative statistics}
  Relation \ddrule{HasFeature} is used to capture quantitative statistics of the input dataset:
  \begin{multline}
    Value?(t,a,d):-\\HasFeature(t,a,f)\ weight=w(d,f)\label{equ:statistics}
  \end{multline}
  To encode the co-occurrences of attribute values \ddrule{HasFeature} stores the values of other cells in the same tuple of a given cell in a feature vector.
  Users can customize \holoclean{} by adding more features to the \ddrule{HasFeature} relation.
  This allows for example to track the provenance of cell values and prioritize data sources differently.
  \holoclean{} parametrizes the weight annotation for the feature inference rule by the value of the cell $d$ and the feature vector $f$ to allow different confidence levels for the combinations of the values of $d$ and $f$.
  Those weights are not set during the generation of the rules itself and can be learned later on.
    
  \subsubsection*{External information}
  Based on external dictionaries and matching dependencies \holoclean{} calculates all matches of the noisy dataset's cells and the values found in the dictionaries for the cells.
  This matches are stored in a new relation \ddrule{Matched(t,a,v,k)}, where $k$ denotes the dictionary which provided the value.
  For each dictionary $k$ and all cells (identified by $t.a$) the relation \ddrule{Matched(t,a,v,k)} looks up the suggested dictionary value.
  This relation is used to create the following inference rule for the external information:
  \begin{equation}
    Value(t,a,d):-Matched(t,a,d,k)\ weight=w(k)\label{equ:matching}
  \end{equation}
  Each dictionary can be trusted more or less and therefore the weight for this rules is parameterized by the dictionary identifier $k$.
  The weights can be learned later on.

  \subsubsection*{Integrity constraints}
  The second input signal are \glspl{ic}.
  They enforce constraints between the values of cells and therefore would connect multiple random variables with a factor in the factor graph.
  
  In a later step \holoclean{} uses Gibbs sampling~\cite{gibbssampling} to perform inference on the factor graph.
  This process iterates over all random variables of the factor graph and samples a value for a single variable from its conditional distribution, while keeping all other variables fixed.
  This approximate inference process is still computationally expensive.
  However, if there are only independent random variables in the factor graph Gibbs sampling requires only $O(n \log n)$ iterations to converge.
  
  Up until now, all introduces inference rules do not cause any dependencies between random variables, because they only contain one variable in their head expression.
  To avoid dependent random variables and utilize the guaranteed runtime of Gibbs sampling, \holoclean{} does not encode \glspl{ic} with multiple random variables in the head of the inference rules as one would expect.
  For each denial constraint \holoclean{} generates multiple inference rules containing one of the involved cell's random variables in the head of the rule and the predicate constraints and the other cell's value mappings in the body.
  Under the following two assumptions this approximated approach can be used:
  \begin{enumerate}
    \item There must exist more clean cells in the dataset than erroneous ones. This assumption represents the already introduced minimality prior.
    \item Each violation of an \gls{ic} should be repairable by changing just one of the involved cells.
  \end{enumerate}
  
  Lets consider the following denial constraint only containing two predicates that represents the functional dependency $t.A \to t.B$ as an example:
  \begin{equation}
    \forall t_1, t_2 \in D: \neg(t_1.A = t_2.A \land t_1.B \neq t_2.B)
  \end{equation}
  Each instance of this constraint involves four cells ($t_1.A$, $t_1.B$, $t_2.A$ and $t_2.B$) of two tuples ($t_1$ and $t_2$).
  The quantifier of denial constraints ($\forall t_1, t_2$) is converted to a self-join of relation \ddrule{Tuple}.
  This means, we only have to consider the cells of one tuple to be in the head of the rules:
  \begin{align}
    \begin{split}
      !Value?&(t_1,A,v_1):-\\
      &InitValue(t_2,A,v_2), InitValue(t_1,B,u_1),\\
      &InitValue(t_2,B,u_2), Tuple(t_1), Tuple(t_2)\\
      &[t_1 \neq t_2, v_1 = v_2, u_1 \neq u_2]\ weight=w%\label{equ:relaxedIC1}
    \end{split}\\
    \begin{split}
      !Value?&(t_1,B,u_1):-\\
      &InitValue(t_2,B,u_2), InitValue(t_1,A,v_1),\\
      &InitValue(t_2,A,v_2), Tuple(t_1), Tuple(t_2)\\
      &[t_1 \neq t_2, v_1 = v_2, u_1 \neq u_2]\ weight=w%\label{equ:relaxedIC1}
    \end{split}
  \end{align}
  The body of the rules then includes all other involved cells via relation \ddrule{InitValue}.
  This relation maps the cells to their initial value and encode the first assumption in \ddlog.
  Additionally, relation \ddrule{Tuple} is used to map the cells to their corresponding tuples and implement the self-join.
  The conditions of the predicates in the \gls{ic} are applied on the cell's values ($v_1$, $v_2$, $u_1$, and $u_2$) and are enhanced with an additional condition preventing the join of a tuple with itself ($t_1 \neq t_2$).
  This template can then be applied to all the allowed combinations of tuples in $D$.
  
  The weight $w$ of each rule specifies how much emphasis is put on fulfilling it.
  Larger weights imply more emphasis on the rule.
  \holoclean{} does not set the weights of those rules to a fixed value, but rather estimates those weights during training of the model.
  
  \bigskip
  After all relations and inference rules were generated, the \ddlog{} program is passed to \deepdive{} to create the factor graph.
  This step will generate all random variables for the cells and the factors between them.
  Some weights are already set by \holoclean{}, others are just represented by variables and will be learned in the next step.
  
  \subsection{Data repairing}
  To repair the dataset $D$ \holoclean{} first has to learn all weights.
  This is done via statistical learning in \deepdive{}.
  Random variables in the factor graph that correspond to the clean cell set $D_c$ are used as labeled examples.
  
  After fixing all weights \holoclean{} now has to infer the correct values for the dirty cells in set $D_d$.
  Therefore, \holoclean{} computes the distribution of all values a random variable can be assigned to.
  For all random variables corresponding to the cells in $D_d$ the marginal probabilities for all possible values are obtained via Bayes inference over the joint distribution.
  As this is a very computational expensive task an approximate approach is used: Gibbs sampling~\cite{gibbssampling}.
  It is a sampling technique to estimate the full posterior probability distributions for all random variables corresponding to the noisy cells.
  
  \holoclean{}'s goal is to output a cleaned dataset.
  The system achieves this goal by assigning the maximum a posteriori estimates as the values of the noisy cells.
  Each repair is automatically annotated with its marginal probability which represents the confidence of the system that this repair is correct~\cite{holoclean}. 
  
  Example: If we consider a cell $c$, its domain $dom(c)=\{v_1, v_2\}$ and the inferred posterior distribution
  \begin{equation}
    P(c) =
    \begin{bmatrix}
      P(c = v_1)\\
      P(c = v_2)
    \end{bmatrix} = 
    \begin{bmatrix}
      0.81\\
      0.19
    \end{bmatrix}
  \end{equation}
  then \holoclean{} proposes the value $v_1$ as a repair for cell $c$ as it is the estimate with the highest marginal probability.
  This means that \holoclean{} is $81\%$ confident about this repair.
  By implication, this also means that 19 out of 100 repairs with probability $0.81$ will not be correct.

\section{Performance of \holoclean{}}\label{sec:performance}
  In this section we summarize the results of the experiments made by \citeauthor{holoclean}.
  We will look at the accuracy of \holoclean{}'s repair suggestions and the runtime of the framework to compute these.
  Both measures are compared to three other state-of-the-art approaches:
  Holistic~\cite{holistic} uses denial constraints to detect and repair errors,
  KATARA~\cite{katara} generates repairs based on external knowledge bases, and
  SCARE~\cite{scare} considers statistics about the dataset to perform machine learning based repairing.
  Before going deeper into the results, we review the datasets used in the evaluation.

  \subsection{Datasets}
  \citeauthor{holoclean} selected four datasets with different sizes and types of errors.
  For all datasets, \holoclean{} was used to repair violations of \glspl{ic}.

  \subsubsection*{Hospital}
  This is a benchmark dataset with about five percent errors and ground truth data available for all cells.
  It consists of 19,000 cells and nine denial constraints.
  Biggest error type are duplicates.

  \subsubsection*{Flights}
  This dataset is also available with ground truth data and consists of 14,262 cells and four denial constraints.
  A large proportion of the cells are erroneous, thus we can use this dataset to evaluate the robustness of \holoclean{}.

  \subsubsection*{Food}
  This food inspection dataset from the City of Chicago is created by manually filled out forms and contains a lot of non-systematic errors, such as misspelled entries or duplicates.
  At the date of usage the dataset contained 5.78 million cells and seven denial constraints that capture the mentioned errors.
  2,000 cells were manually labeled based on detections of Holistic, KATARA, and SCARE.

  \subsubsection*{Physicians}
  The last dataset is the Physician Compare National dataset, which contains a lot of systematic errors due to misspellings or inconsistencies.
  This dataset contains 37.29 million cells and is the biggest one used for evaluation.
  Nine denial constraints are used to capture the errors in the dataset.
  The same technique than for the \textit{Food} dataset was used to obtain 2,500 labeled cells.

  \subsection{Accuracy of repairs}
  We first look at the accuracy of the suggested repairs compared to the other three approaches.
  The accuracy is reported based on the $F_1$-score:
  \begin{equation}
    F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}
  \end{equation}
  The results for the different datasets are presented in Figure~\ref{fig:accuracy}.
  For \holoclean{} the following thresholds $\tau$ for pruning the domain of random variables were used:
  Hospital $\tau = 0.5$, Flights $\tau = 0.3$, Food $\tau = 0.5$, and Physicians $\tau = 0.7$.
  %\begin{description}
  %  \item[Hospital] $\tau = 0.5$
  %  \item[Flights] $\tau = 0.3$
  %  \item[Food] $\tau = 0.5$
  %  \item[Physicians] $\tau = 0.7$
  %\end{description}
  Missing values (zero bars) in the diagram mean that no repairs were performed (Holistic and KATARA) or the process did not terminate after three days (SCARE).
  \holoclean{} outperforms the other state-of-the-art approaches in all four datasets significantly.
  The $F_1$-score of \holoclean{} is more than $0.2$ points higher than the one of the other approaches in all cases.
  Even on the \textit{Flights} dataset, which has the highest error concentration, \holoclean{} could reach an accuracy of $76\%$.
  This dataset was the hardest to fix, as we can deduce on the basis that Holistic and KATARA did not repair anything and SCARE only reached an accuracy of $10\%$.
  
  \begin{figure}
    \centering
    \includestandalone{tikz/accuracy}
    \caption{Accuracy ($F_1$-score) of \holoclean{} compared to other approaches. Obtained from~\cite{holoclean}}
    \label{fig:accuracy}
  \end{figure}

  This shows that the holistic approach used by \holoclean{} performs repairs with a significantly higher accuracy compared to other state-of-the-art approaches that only use one input signal.

  \subsection{Runtime of \holoclean{}}
  
  \begin{table}
    \caption{Runtime of \holoclean{} compared to other approaches. Obtained from~\cite{holoclean}}
    \label{tab:runtime}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l|cccc}
      \toprule
      \textbf{Dataset} & \textbf{\holoclean{}} & \textbf{Holistic} & \textbf{KATARA} & \textbf{SCARE}\\
      \midrule
      Hospital & 148.0 sec & 5.7 sec & 2.0 sec & 24.7 sec \\
      Flights & 70.6 sec & 80.4 sec & n/a & 14.0 sec \\
      Food & 32.8 min & 7.6 min & 1.7 min & not term. \\
      Physicians & 6.5 hours & 2.0 hours & 15.5 min & not term. \\
      \bottomrule
    \end{tabular}
  }
  \end{table}

\section{Conclusion}\label{sec:conclusion}